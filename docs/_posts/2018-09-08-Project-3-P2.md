---
layout: post
title: The Scraping of Reddit, Part 2
---
This part of the project will be to get the data that we collected over the coarse of a few hours. We will clean up the data and see what graphs and charts we can extract to make the data easier to read.

First thing we do is pull the data from the csv files.


```python
import numpy as np
import pandas as pd

```

Now we read the csv for RBN  and combine the respective dataframes into a single dataframe.


```python
df1 = pd.read_csv('./2018-09-05_10:14:22_hot_raisedbynarcissists.csv')
df2 = pd.read_csv('./2018-09-05_00:14:59_hot_raisedbynarcissists.csv')
df3 = pd.read_csv('./2018-09-04_20:47:06_hot_raisedbynarcissists.csv')
df4 = pd.read_csv('./2018-09-04_18:05:51_hot_raisedbynarcissists.csv')
#df5 = pd.read_csv('./2018-09-04_17:44:38_hot_raisedbynarcissists.csv')

```


```python
df_RBN=pd.concat([df1,df2,df3,df4], sort=False)
```



We do the same thing with the data from JNM


```python
df1 = pd.read_csv('./2018-09-05_10:12:28_hot_JUSTNOMIL.csv')
df2 = pd.read_csv('./2018-09-05_00:13:13_hot_JUSTNOMIL.csv')
df3 = pd.read_csv('./2018-09-04_20:45:42_hot_JUSTNOMIL.csv')
df4 = pd.read_csv('./2018-09-04_17:42:45_hot_JUSTNOMIL.csv')
```


```python
df_JNM = pd.concat([df1,df2,df3,df4], sort=False)
```



Time to drop the duplicates


```python
df_RBN.drop_duplicates(['title'], inplace = True)
df_JNM.drop_duplicates(['title'], inplace=True)
```





Now we merge the two dataframes together and save it.


```python
df_final = pd.concat([df_RBN,df_JNM],sort=False)
```


```python
df_final.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 349 entries, 0 to 99
    Data columns (total 13 columns):
    Unnamed: 0           349 non-null int64
    title                349 non-null object
    comments             349 non-null object
    author               349 non-null object
    guilded              349 non-null int64
    num_comments         349 non-null int64
    score                349 non-null int64
    pinned               349 non-null bool
    subreddit            349 non-null object
    author_flair_text    19 non-null object
    created_utc          349 non-null float64
    r_id                 349 non-null object
    body                 329 non-null object
    dtypes: bool(1), float64(1), int64(4), object(7)
    memory usage: 35.8+ KB


Lets do a bit of cleanup

- unnamed:0  was a mistake on my part.
- comments is useless now and cant be read.


Unfortunately, when I scraped the data I didn't fully understand how to extract comments.


```python
df_final.drop(['Unnamed: 0','comments'],axis=1, inplace=True)
df_final.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 349 entries, 0 to 99
    Data columns (total 11 columns):
    title                349 non-null object
    author               349 non-null object
    guilded              349 non-null int64
    num_comments         349 non-null int64
    score                349 non-null int64
    pinned               349 non-null bool
    subreddit            349 non-null object
    author_flair_text    19 non-null object
    created_utc          349 non-null float64
    r_id                 349 non-null object
    body                 329 non-null object
    dtypes: bool(1), float64(1), int64(3), object(6)
    memory usage: 30.3+ KB


Going to randomize the data before I save it


```python
df_final = df_final.sample(frac=1).reset_index(drop=True)
```




```python
df_final.to_csv('./Data/Final.csv')
```

While we're here lets get some word clouds


```python
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
```

RBN title word cloud


```python
#df = pd.read_csv('./Data/Final.csv')

comment_words = ' '
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in df_RBN.title:

    # typecaste each val to string
    val = str(val)

    # split the value
    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    for words in tokens:
        comment_words = comment_words + words + ' '


wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()
```


![png](/blog/docs/assets/images/project_3/output_27_0.png)


RBN body word cloud


```python

comment_words = ' '
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in df_RBN.body:

    # typecaste each val to string
    val = str(val)

    # split the value
    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    for words in tokens:
        comment_words = comment_words + words + ' '


wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()
```


![png](/blog/docs/assets/images/project_3/output_29_0.png)


JNM title word cloud


```python
comment_words = ' '
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in df_JNM.title:

    # typecaste each val to string
    val = str(val)

    # split the value
    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    for words in tokens:
        comment_words = comment_words + words + ' '


wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()
```


![png](/blog/docs/assets/images/project_3/output_31_0.png)


JNM body word cloud


```python

comment_words = ' '
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in df_JNM.body:

    # typecaste each val to string
    val = str(val)

    # split the value
    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    for words in tokens:
        comment_words = comment_words + words + ' '


wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()
```


![png](/blog/docs/assets/images/project_3/output_33_0.png)



```python
import seaborn as sns
```

Comments in RBN. The majority of posts have less than 50 comments with a few rare exceptions.


```python
plt.scatter(df_RBN.index, df_RBN.num_comments) #rbn
```




    <matplotlib.collections.PathCollection at 0x7f4690313d68>




![png](/blog/docs/assets/images/project_3/output_36_1.png)


The two outliers are for a post titled "My wife just savagely put my Ndad in his place" which looks like the closest thing to a feel good story and "When your Nparents start losing control of you they tell you "you've changed"" which is a discussion post that got a bunch of traction. I wish I was able to get the comments for these posts but it was not meant to be.


```python
print(df_RBN.title[df_RBN.num_comments>250].iloc[1])
```

    When your Nparents start losing control of you they tell you "you've changed"


Comments in JNM. The majority of comments are also below 50 but a significant number are above 100 showing more participation in the subreddit. The largest post is titled "'At 16 weeks pregnant, I was attacked by my In-laws. Both myself and my daughters lives put in danger. Now my husband wants us to have a relationship with them again. What should I do?'" which is a very dark sounding thread. Advice posts do tend to get a lot of traction.


```python
plt.scatter(df_JNM.index, df_JNM.num_comments) #jnm
```




    <matplotlib.collections.PathCollection at 0x7f65170ae748>




![png](/blog/docs/assets/images/project_3/output_40_1.png)



```python
df_JNM.title[df_JNM.num_comments>375].iloc[0]
```




    'At 16 weeks pregnant, I was attacked by my In-laws. Both myself and my daughters lives put in danger. Now my husband wants us to have a relationship with them again. What should I do?'



The time that posts are collected in utc. As you can see the two outliers really skew our data. Closer inspection shows us that these are two posts that mods put up. One is titled "'Do you care about this community? Would you like to help us keep it going? Apply to be a mod!'" and the other is "'Welcome new subscribers - read this before posting or commenting!'" These values can safely be dropped.


```python
plt.scatter(df_RBN.index, df_RBN.created_utc) #rbn
```




    <matplotlib.collections.PathCollection at 0x7f469007b048>




![png](/blog/docs/assets/images/project_3/output_43_1.png)



```python
df_RBN.title[df_RBN.created_utc<1.53e9].iloc[1]
```




    'Welcome new subscribers - read this before posting or commenting!'




```python
df_temp = df_RBN[df_RBN.created_utc>1.53e9]
```


```python
df_temp.shape
```




    (194, 13)



Dropping the values gives a much better and more accurate plot.


```python
plt.scatter(df_temp.index, df_temp.created_utc)
```




    <matplotlib.collections.PathCollection at 0x7f650dafce80>




![png](/blog/docs/assets/images/project_3/output_48_1.png)


The scatter plot of JNM shows a much more dynamic subreddit. Removing posts is not necessary for this data.


```python
plt.scatter(df_JNM.index, df_JNM.created_utc) # jnm
```




    <matplotlib.collections.PathCollection at 0x7f651659a1d0>




![png](/blog/docs/assets/images/project_3/output_50_1.png)
