---
layout: post
title: The Scraping of Reddit, Part 3
---

The goal here is to take the data that I gathered in part 1 and worked on in part 2 and attempt to model it. The plank is to run the data through a TFIDF Vectorizor. As we saw in the last part with the word clouds both subreddits have their own verbiage and TFIDF will give these words greater weight making predictions more accurate.  The plan is then going to be to compare some models starting with logistic regression as a baseline. This is going to be a classification problem as there are only two possible outcomes.

Fiirst, importing everything.


```python
import numpy as np
import pandas as pd
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
lemmatizer = WordNetLemmatizer()
import string
```

This function is to remove punctuation. I know TFIDF can do it but I wanted practice with .append.


```python
def remove_punctuation(txt):
    for punctuation in string.punctuation:
        txt = txt.replace(punctuation, '')
    return txt
```


```python
import regex as re
```

Read in the data that we did EDA with


```python
df = pd.read_csv('./Data/Final.csv')

```


The .apply works


```python
df.title = df.title.apply(remove_punctuation)
```

I know it's my data but we still have to do a bit more EDA.


```python
df.body.fillna('',inplace=True)
df.body = df.body.apply(remove_punctuation)
df.author_flair_text.fillna('',inplace=True)
```

Time to change the subreddits names to 1 and 0 so the models can work.


```python
df.subreddit=np.where(df.subreddit=='JUSTNOMIL',1,0)
```

I don't have enough samples. Lets pump up this data set by a factor of 10 with bootstraping.


```python
df = df.sample(n = 4000, replace=True)
```


```python
from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer
```


```python
from sklearn.model_selection import train_test_split
```

First we need to train test split. This separates the data into a training set and a testing set. We first split the data along the post title.  


```python
X_train, X_test, y_train, y_test = train_test_split(df.title,
                                        df.subreddit,test_size = .2, random_state = 42)
```

Now we run the TFIDF. This separates all the words in the title and gives them a value based on how frequently the word appears.


```python
tvec = TfidfVectorizer(stop_words='english')

X_train_vec = tvec.fit_transform(X_train)
X_test_vec = tvec.transform(X_test)
```


```python
tvec.vocabulary_
```

A small example of the words and weights in TFIDF.


    {'really': 792,
     'worried': 1059,
     'idk': 461,
     'laws': 523,
     'raising': 780,
     'grandkids': 394,
     'dh': 234,
     'wants': 1034,
     'live': 549,
     'near': 654,
     'eventually': 298,
     'womb': 1055,
     'landlords': 519,
     'emotional': 285,
     'waste': 1038,
     'bin': 87,
     'tells': 947,
     ...}




```python
df_tvec = pd.DataFrame(list(tvec.vocabulary_))
```



```python
df_X_train = pd.DataFrame(X_train_vec.toarray(), columns=tvec.get_feature_names())
df_X_test = pd.DataFrame(X_test_vec.toarray(), columns=tvec.get_feature_names())
```






```python
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
model = logreg.fit(df_X_train, y_train)
```

The logreg model works really well. Probably because of the way the subreddit uses it's own verbiage


```python
logreg.score(df_X_test, y_test)
```




    1.0



Let's run this with the body messages


```python
X_train, X_test, y_train, y_test = train_test_split(df.body,
                                        df.subreddit,test_size = .2, random_state = 42)
```


```python
tvec = TfidfVectorizer(stop_words='english')

X_train_vec = tvec.fit_transform(X_train)
X_test_vec = tvec.transform(X_test)
```


```python
df_X_train = pd.DataFrame(X_train_vec.toarray(), columns=tvec.get_feature_names())
df_X_test = pd.DataFrame(X_test_vec.toarray(), columns=tvec.get_feature_names())
```







```python
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
model = logreg.fit(df_X_train, y_train)
```


```python
logreg.score(df_X_test, y_test)
```




    0.98875



That was still really good.


```python
from sklearn.metrics import accuracy_score
```


```python
cleo = logreg.predict(df_X_test)
```


```python
accuracy_score(y_test,cleo)
```




    0.98875



That's really accurate. Lets try some different models


```python
from sklearn.ensemble import RandomForestClassifier
```


```python
rfc=RandomForestClassifier()
rfc.fit(df_X_train, y_train)
```




    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=1, min_samples_split=2,
                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
                oob_score=False, random_state=None, verbose=0,
                warm_start=False)




```python
rfc.score(df_X_test, y_test)
```




    0.98875




```python
df_rfc  = pd.DataFrame(rfc.feature_importances_,
                              tvec.get_feature_names(),
                              columns=['Feature'])
```


```python
temp_df = df_rfc.sort_values(by='Feature', ascending=False)[:20]
```


```python
temp_df.iloc[:, ::-1].plot(kind='barh', figsize = (8,8), color= 'b')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f307eb33f98>




![png](/blog/docs/assets/images/project_3/output_51_1.png)
